{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "count seems to decrease score\n",
    "full interactions seems too much\n",
    "\n",
    "features, shape, iter, cv, public\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import vw_utils as vw\n",
    "import gini\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/train.csv')\n",
    "test = pd.read_csv('../data/test.csv')\n",
    "\n",
    "selected = np.random.rand(train.shape[0]) < 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------------------- Feature Start -----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# raw features\n",
    "ID = test.Id.values\n",
    "\n",
    "categorical = ['T1_V' + str(i) for i in list(range(4, 10)) + [11, 12, 15, 16, 17]] + \\\n",
    "                ['T2_V' + str(i) for i in [3, 5, 11, 12, 13]]\n",
    "\n",
    "numerical = set(train.columns).difference(categorical + ['Hazard', 'Id'])\n",
    "numerical = list(numerical)\n",
    "\n",
    "train_c = train.copy()\n",
    "test_c = test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-bfc2386a5df3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0msofia\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msofia_kmeans\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m60000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0msofia\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnumerical\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/albertium/kaggle/liberty_mutual_property/scripts/sofia_utils.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m                 \u001b[0m_pd2svm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'train.libsvm'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'train.libsvm'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m                         \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/albertium/kaggle/liberty_mutual_property/scripts/sofia_utils.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, filename)\u001b[0m\n\u001b[0;32m     33\u001b[0m                                 \u001b[1;33m+\u001b[0m \u001b[1;34m' --training_file '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m                                 \u001b[1;33m+\u001b[0m \u001b[1;34m' --model_out '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'model'\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m                                 \u001b[1;33m+\u001b[0m \u001b[1;34m' --dimensionality '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommand_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m                 \u001b[0mflag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommand_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshell\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'out'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "11898600.0\n",
    "11795000.0\n",
    "11914800.0\n",
    "11926600.0\n",
    "\"\"\"\n",
    "\n",
    "# sofia feature\n",
    "from sofia_utils import sofia_kmeans\n",
    "\n",
    "sofia = sofia_kmeans(n_clusters=6, batch_size=60000)\n",
    "sofia.fit(train[numerical])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50999, 16)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# kmeans feature\n",
    "from numpy.linalg import eig\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "means = train_c[numerical].mean()\n",
    "e = (train_c[numerical].max() - train_c[numerical].min()) * 0.04\n",
    "stds = np.sqrt(train_c[numerical].var() + e)\n",
    "\n",
    "X = (train_c[numerical] - means) / stds\n",
    "cov = np.cov(X.values.T)\n",
    "D, V = eig(cov)\n",
    "D_isqrt = np.diag(1 / np.sqrt(D + 0.01))\n",
    "\n",
    "X_whiten = np.dot(X, np.dot(np.dot(V, D_isqrt), V.T))\n",
    "\n",
    "X_whiten.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50999, 111)\n",
      "CPU times: user 9.19 s, sys: 116 ms, total: 9.3 s\n",
      "Wall time: 9.31 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/albertium/anaconda3/lib/python3.4/site-packages/sklearn/utils/validation.py:498: UserWarning: StandardScaler assumes floating point values as input, got int64\n",
      "  \"got %s\" % (estimator, X.dtype))\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# normalize features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "# normalize numerical features\n",
    "SS = StandardScaler()\n",
    "train_c[numerical] = SS.fit_transform(train_c[numerical])\n",
    "test_c[numerical] = SS.transform(test_c[numerical])\n",
    "\n",
    "# digitize categorical features\n",
    "DV = DictVectorizer(sparse=False)\n",
    "train_c = DV.fit_transform(train_c[numerical+categorical].T.to_dict().values())\n",
    "test_c = DV.transform(test_c[numerical+categorical].T.to_dict().values())\n",
    "\n",
    "print(train_c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "StandardScaler??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50999, 6216)\n",
      "CPU times: user 6.03 s, sys: 2.56 s, total: 8.59 s\n",
      "Wall time: 8.59 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "PF = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "train_c = PF.fit_transform(train_c)\n",
    "print(train_c.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------------------- Feature End -----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.00621682827474\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "import gini\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "gini.normalized_gini??\n",
    "\n",
    "regressor = SGDRegressor(loss='squared_loss', penalty='l1', alpha=1E-7, n_iter=10)\n",
    "scores = cross_val_score(regressor, train_c, train.Hazard.values, cv=20, scoring=gini.normalized_gini_score)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# 0.36597\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "RFR = RandomForestRegressor(n_estimators=10000, max_features='sqrt', min_samples_leaf=20, oob_score=True, n_jobs=-1)\n",
    "scores = cross_val_score(RFR, train_c, train.Hazard.values, cv=20, scoring=gini.normalized_gini_score, n_jobs=-1)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = 'linear1'\n",
    "yhat = np.loadtxt('../tmp/{}/out'.format(model))\n",
    "pd.DataFrame({'Id': ID, 'Hazard': yhat}).reindex_axis(['Id', 'Hazard'], 1).to_csv('../output/lin01.csv', index=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------------------- Factory -----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.5 s, sys: 4 ms, total: 14.5 s\n",
      "Wall time: 14.5 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/albertium/anaconda3/lib/python3.4/site-packages/sklearn/utils/validation.py:498: UserWarning: StandardScaler assumes floating point values as input, got int64\n",
      "  \"got %s\" % (estimator, X.dtype))\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# kmeans features\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "SS = StandardScaler()\n",
    "\n",
    "kmeans = KMeans(80, random_state=12345, n_init=1)\n",
    "feat_kmeans_train = kmeans.fit_transform(SS.fit_transform(train[numerical]))\n",
    "feat_kmeans_test = kmeans.transform(SS.transform(test[numerical]))\n",
    "\n",
    "# print('variance lost:')\n",
    "# print(kmeans.inertia_ / KMeans(1, random_state=12345, n_init=1).fit(train[numerical]).inertia_)\n",
    "\n",
    "feat_kmeans_train = pd.DataFrame(feat_kmeans_train).add_prefix('kmeans_')\n",
    "feat_kmeans_test = pd.DataFrame(feat_kmeans_test).add_prefix('kmeans_')\n",
    "\n",
    "numerical += feat_kmeans_train.columns.values.tolist()\n",
    "train_c = pd.concat([train_c, feat_kmeans_train], axis=1)\n",
    "test_c = pd.concat([test_c, feat_kmeans_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# count features\n",
    "\n",
    "feat_count_train = train[categorical].copy()\n",
    "feat_count_test = test[categorical].copy()\n",
    "\n",
    "for cat in categorical:\n",
    "    numerical.append('cnt_'+cat)\n",
    "    hotDeck = train[cat].value_counts().reset_index().rename(columns={'index': cat, 0: 'cnt_'+cat})\n",
    "    feat_count_train = pd.merge(feat_count_train, hotDeck, on=cat)\n",
    "    del feat_count_train[cat]\n",
    "    \n",
    "    feat_count_test = pd.merge(feat_count_test, hotDeck, on=cat)\n",
    "    del feat_count_test[cat]\n",
    "    \n",
    "train_c = pd.concat([train_c, feat_count_train], axis=1)\n",
    "test_c = pd.concat([test_c, feat_count_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "encode_T1_V4     1.011131\n",
       "encode_T1_V5     1.003089\n",
       "encode_T1_V6     0.999460\n",
       "encode_T1_V7     1.025445\n",
       "encode_T1_V8     0.985815\n",
       "encode_T1_V9     1.000703\n",
       "encode_T1_V11    1.001235\n",
       "encode_T1_V12    0.987670\n",
       "encode_T1_V15    1.007230\n",
       "encode_T1_V16    1.005126\n",
       "encode_T1_V17    1.003132\n",
       "encode_T2_V3     0.998128\n",
       "encode_T2_V5     1.001219\n",
       "encode_T2_V11    1.002282\n",
       "encode_T2_V12    1.004119\n",
       "encode_T2_V13    0.994556\n",
       "dtype: float64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# categorical encoding\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "feat_encode_train = train[categorical].copy()\n",
    "feat_encode_test = test[categorical].copy()\n",
    "\n",
    "for cat in categorical:\n",
    "    hot_deck = train.groupby(cat).Hazard.mean().reset_index().rename(columns={'Hazard': 'encode_'+cat})\n",
    "    feat_encode_train = pd.merge(feat_encode_train, hot_deck, on=cat)\n",
    "    del feat_encode_train[cat]\n",
    "    \n",
    "    feat_encode_test = pd.merge(feat_encode_test, hot_deck, on=cat)\n",
    "    del feat_encode_test[cat]\n",
    "    \n",
    "names = feat_encode_train.columns\n",
    "ss = StandardScaler()\n",
    "feat_encode_train[names] = ss.fit_transform(feat_encode_train.values)\n",
    "feat_encode_test[names] = ss.transform(feat_encode_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6.86 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# reduce\n",
    "from xgboost import XGBRegressor\n",
    "import xgboost as xgb\n",
    "\n",
    "feat_reduce_train = pd.DataFrame()\n",
    "feat_reduce_test = pd.DataFrame()\n",
    "\n",
    "for num in numerical:\n",
    "    dTmp = xgb.DMatrix(train[[num]], label=train.Hazard)\n",
    "    reg = xgb.train({'max_depth':8, 'min_child_weight':30, 'objective':'reg:linear'}, dTmp, 1)\n",
    "    \n",
    "    feat_reduce_train['discrete_'+num] = reg.predict(dTmp, pred_leaf=True)\n",
    "    feat_reduce_train['discrete_'+num] = feat_reduce_train['discrete_'+num].map(str)\n",
    "    \n",
    "    feat_reduce_test['discrete_'+num] = reg.predict(xgb.DMatrix(test[[num]]), pred_leaf=True)\n",
    "    feat_reduce_test['discrete_'+num] = feat_reduce_test['discrete_'+num].map(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# discrete\n",
    "\n",
    "feat_discrete_train = train[numerical].applymap(str)\n",
    "feat_discrete_test = test[numerical].applymap(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# combine all features\n",
    "cTrain = train[numerical+categorical]\n",
    "cTest = test[numerical+categorical]\n",
    "\n",
    "# cTrain = pd.concat([train[numerical+categorical], feat_reduce_train], axis=1)\n",
    "# cTest = pd.concat([test[numerical+categorical], feat_reduce_test], axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
