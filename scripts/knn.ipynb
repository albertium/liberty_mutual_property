{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n01, raw xgb + raw RF, simple average, 0.378522\\n02, raw xgb + raw RF, rank average, 0.377719\\n03, raw xgb, 0.381522 (works fine!)\\n04, raw RF, 0.365994\\n05, raw xgb + RF, rank multiplication, 0.380349 (works better than I thought)\\n06, should i try rank min, 0.380458\\n07, stack RF, Extra, XGB, 0.382054\\n08, xgb (subsample 0.8), 0.382557\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "01, encoding, 30, 0.30\n",
    "02, encoding, 64, 0.31596\n",
    "03, dummy2 (xgb for each col), 30, 0.26101\n",
    "04, dummy3 (raw + xgb for all col), 30, 0.301124\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ncategorical encoding + knn\\nxgb features + knn\\ncategorical only + various estimator\\nnumerical only + various estimator\\nvariable selection (xgb, RF) + various estimator\\nnumerical as categorical\\nsecond order features\\nstandardize kfold transformer\\nnumerical as categorical + 2nd order + linear\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "categorical encoding + knn\n",
    "xgb features + knn\n",
    "categorical only + various estimator\n",
    "numerical only + various estimator\n",
    "variable selection (xgb, RF) + various estimator\n",
    "numerical as categorical\n",
    "second order features\n",
    "standardize kfold transformer\n",
    "numerical as categorical + 2nd order + linear\n",
    "how to incorporate the info T1 vs T2 into modeling?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import vw_utils as vw\n",
    "import gini\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/train.csv')\n",
    "test = pd.read_csv('../data/test.csv')\n",
    "\n",
    "train_set = np.hstack([np.ones(train.shape[0]), np.zeros(test.shape[0])])\n",
    "\n",
    "data = pd.concat([train, test], axis=0).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------------------- Feature Start -----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# start\n",
    "categorical = ['T1_V' + str(i) for i in list(range(4, 10)) + [11, 12, 15, 16, 17]] + \\\n",
    "                ['T2_V' + str(i) for i in [3, 5, 11, 12, 13]]\n",
    "\n",
    "numerical = set(train.columns).difference(categorical + ['Hazard', 'Id'])\n",
    "numerical = list(numerical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101999, 111)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# raw feature\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import re\n",
    "\n",
    "SS = StandardScaler()\n",
    "feat_raw = data[numerical].applymap(float)\n",
    "feat_raw[numerical] = SS.fit_transform(feat_raw[numerical])\n",
    "feat_raw_cat = pd.get_dummies(data[categorical])\n",
    "\n",
    "feat_raw = pd.concat([feat_raw, feat_raw_cat], axis=1)\n",
    "feat_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101999, 168)\n",
      "CPU times: user 3.94 s, sys: 88 ms, total: 4.02 s\n",
      "Wall time: 3.54 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# dummy 3\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "import xgboost as xgb\n",
    "\n",
    "feat_dummy = pd.DataFrame()\n",
    "\n",
    "\n",
    "dTmp = xgb.DMatrix(feat_raw[train_set==1], label=train.Hazard[train_set==1])\n",
    "reg = xgb.train({'max_depth':7, 'min_child_weight':100, 'objective':'reg:linear'}, dTmp, 3)\n",
    "\n",
    "dTmp = xgb.DMatrix(feat_raw)\n",
    "feat_dummy = reg.predict(dTmp, pred_leaf=True)\n",
    "feat_dummy = pd.DataFrame(feat_dummy, columns=['dummy_'+str(i) for i in range(3)])\n",
    "\n",
    "for x in feat_dummy.columns:\n",
    "    feat_dummy[x] = feat_dummy[x].astype('category')    \n",
    "\n",
    "feat_dummy = pd.get_dummies(feat_dummy)\n",
    "print(feat_dummy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50999, 279)\n",
      "(51000, 279)\n"
     ]
    }
   ],
   "source": [
    "# assemble features\n",
    "\n",
    "tmp = pd.concat([feat_raw, feat_dummy], axis=1)\n",
    "train_c = tmp[train_set==1]\n",
    "test_c = tmp[train_set==0]\n",
    "\n",
    "print(train_c.shape)\n",
    "print(test_c.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------------------- Modeling -----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.301124301553\n",
      "CPU times: user 17min 43s, sys: 188 ms, total: 17min 44s\n",
      "Wall time: 17min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# knn 10\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from wrappers import cross_validate\n",
    "\n",
    "knn = KNeighborsRegressor(30)\n",
    "score = cross_validate(knn, train_c, train.Hazard, nfold=5)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame({'Id': test.Id, 'Hazard': yhat}).reindex_axis(['Id', 'Hazard'], 1).to_csv('../output/ensemble_07.csv', index=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------------------- Test Ground -----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------------------- Factory -----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# categorical encoding\n",
    "from wrappers import Whitener\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "feat_encode = data[categorical].copy()\n",
    "for cat in categorical:\n",
    "    hot_deck = data.groupby(cat).Hazard.mean().to_dict()\n",
    "    feat_encode[cat] = feat_encode[cat].apply(lambda x: hot_deck[x])\n",
    "\n",
    "feat_encode = feat_encode.rename(columns=lambda x: 'encode_'+x)\n",
    "feat_encode = pd.concat([feat_encode, data[numerical]], axis=1)\n",
    "\n",
    "# feat_encode[feat_encode.columns] = StandardScaler().fit_transform(feat_encode)\n",
    "feat_encode[feat_encode.columns] = Whitener().fit_transform(feat_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# dummy 2\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "import xgboost as xgb\n",
    "\n",
    "feat_dummy = pd.DataFrame()\n",
    "\n",
    "for x in numerical:\n",
    "    dTmp = xgb.DMatrix(data.ix[train_set==1, [x]], label=train.Hazard[train_set==1])\n",
    "    reg = xgb.train({'max_depth':100, 'min_child_weight':30, 'objective':'reg:linear'}, dTmp, 1)\n",
    "    \n",
    "    dTmp = xgb.DMatrix(data[[x]], label=train.Hazard)\n",
    "    feat_dummy['dummy_'+x] = reg.predict(dTmp, pred_leaf=True)\n",
    "    feat_dummy['dummy_'+x] = feat_dummy['dummy_'+x].astype('category')\n",
    "    \n",
    "feat_dummy = pd.concat([feat_dummy, data[categorical]], axis=1)\n",
    "feat_dummy = pd.get_dummies(feat_dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dummy \n",
    "feat_dummy = data[numerical+categorical].copy()\n",
    "for x in feat_dummy.columns:\n",
    "    feat_dummy[x] = feat_dummy[x].astype('category')\n",
    "    \n",
    "feat_dummy = pd.get_dummies(feat_dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extract percent:  853192.0\n",
      "sparcity:  0.695033872881\n",
      "(101999, 100)\n",
      "CPU times: user 6.6 s, sys: 1.11 s, total: 7.71 s\n",
      "Wall time: 14.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# sofia feature\n",
    "from sofia_utils import sofia_kmeans\n",
    "\n",
    "n_clusters = 100\n",
    "\n",
    "sofia = sofia_kmeans(n_clusters=n_clusters, iterations=1000, mapping_threshold=0.0001)\n",
    "\n",
    "feat_sofia = sofia.fit_transform(data[numerical])\n",
    "\n",
    "feature_names = ['sofia_'+str(i+1) for i in range(n_clusters)]\n",
    "feat_sofia = pd.DataFrame(feat_sofia, columns=feature_names)\n",
    "print(feat_sofia.shape)\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "MMS = MinMaxScaler()\n",
    "feat_sofia.ix[:, :] = MMS.fit_transform(feat_sofia.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50999, 6216)\n",
      "CPU times: user 6.03 s, sys: 2.56 s, total: 8.59 s\n",
      "Wall time: 8.59 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "PF = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "train_c = PF.fit_transform(train_c)\n",
    "print(train_c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.5 s, sys: 4 ms, total: 14.5 s\n",
      "Wall time: 14.5 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/albertium/anaconda3/lib/python3.4/site-packages/sklearn/utils/validation.py:498: UserWarning: StandardScaler assumes floating point values as input, got int64\n",
      "  \"got %s\" % (estimator, X.dtype))\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# kmeans features\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "SS = StandardScaler()\n",
    "\n",
    "kmeans = KMeans(80, random_state=12345, n_init=1)\n",
    "feat_kmeans_train = kmeans.fit_transform(SS.fit_transform(train[numerical]))\n",
    "feat_kmeans_test = kmeans.transform(SS.transform(test[numerical]))\n",
    "\n",
    "# print('variance lost:')\n",
    "# print(kmeans.inertia_ / KMeans(1, random_state=12345, n_init=1).fit(train[numerical]).inertia_)\n",
    "\n",
    "feat_kmeans_train = pd.DataFrame(feat_kmeans_train).add_prefix('kmeans_')\n",
    "feat_kmeans_test = pd.DataFrame(feat_kmeans_test).add_prefix('kmeans_')\n",
    "\n",
    "numerical += feat_kmeans_train.columns.values.tolist()\n",
    "train_c = pd.concat([train_c, feat_kmeans_train], axis=1)\n",
    "test_c = pd.concat([test_c, feat_kmeans_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# count features\n",
    "\n",
    "feat_count_train = train[categorical].copy()\n",
    "feat_count_test = test[categorical].copy()\n",
    "\n",
    "for cat in categorical:\n",
    "    numerical.append('cnt_'+cat)\n",
    "    hotDeck = train[cat].value_counts().reset_index().rename(columns={'index': cat, 0: 'cnt_'+cat})\n",
    "    feat_count_train = pd.merge(feat_count_train, hotDeck, on=cat)\n",
    "    del feat_count_train[cat]\n",
    "    \n",
    "    feat_count_test = pd.merge(feat_count_test, hotDeck, on=cat)\n",
    "    del feat_count_test[cat]\n",
    "    \n",
    "train_c = pd.concat([train_c, feat_count_train], axis=1)\n",
    "test_c = pd.concat([test_c, feat_count_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6.86 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# reduce\n",
    "from xgboost import XGBRegressor\n",
    "import xgboost as xgb\n",
    "\n",
    "feat_reduce_train = pd.DataFrame()\n",
    "feat_reduce_test = pd.DataFrame()\n",
    "\n",
    "for num in numerical:\n",
    "    dTmp = xgb.DMatrix(train[[num]], label=train.Hazard)\n",
    "    reg = xgb.train({'max_depth':8, 'min_child_weight':30, 'objective':'reg:linear'}, dTmp, 1)\n",
    "    \n",
    "    feat_reduce_train['discrete_'+num] = reg.predict(dTmp, pred_leaf=True)\n",
    "    feat_reduce_train['discrete_'+num] = feat_reduce_train['discrete_'+num].map(str)\n",
    "    \n",
    "    feat_reduce_test['discrete_'+num] = reg.predict(xgb.DMatrix(test[[num]]), pred_leaf=True)\n",
    "    feat_reduce_test['discrete_'+num] = feat_reduce_test['discrete_'+num].map(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# discrete\n",
    "\n",
    "feat_discrete_train = train[numerical].applymap(str)\n",
    "feat_discrete_test = test[numerical].applymap(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# combine all features\n",
    "cTrain = train[numerical+categorical]\n",
    "cTest = test[numerical+categorical]\n",
    "\n",
    "# cTrain = pd.concat([train[numerical+categorical], feat_reduce_train], axis=1)\n",
    "# cTest = pd.concat([test[numerical+categorical], feat_reduce_test], axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
